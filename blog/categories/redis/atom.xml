<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: redis | jjasghar rants and ramblings]]></title>
  <link href="http://jjasghar.github.io/blog/categories/redis/atom.xml" rel="self"/>
  <link href="http://jjasghar.github.io/"/>
  <updated>2014-05-02T11:56:34-05:00</updated>
  <id>http://jjasghar.github.io/</id>
  <author>
    <name><![CDATA[JJ Asghar]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Adventures migrating from 2.4.16 redis to 2.6.16 redis (Part 2)]]></title>
    <link href="http://jjasghar.github.io/blog/2013/10/01/adventures-migrating-from-2-dot-4-16-redis-to-2-dot-6-16-redis-part-2/"/>
    <updated>2013-10-01T18:11:00-05:00</updated>
    <id>http://jjasghar.github.io/blog/2013/10/01/adventures-migrating-from-2-dot-4-16-redis-to-2-dot-6-16-redis-part-2</id>
    <content type="html"><![CDATA[<h1>Monitoring</h1>

<p>Everyone loves monitoring. Ok, that&rsquo;s a lie, only [DevOps|Sysadmins] love monitoring. Ok, that&rsquo;s probably a lie too, hell I love monitoring, and I love nagios.  I&rsquo;ve followed used nagios in some shape or another since nagios-1.0b6. My first real paid job as an &ldquo;IT guy&rdquo; was setting up nagios for a company.  I&rsquo;ve actually gotten in the habit of walking into a new company using <a href="http://jjasghar.github.io/blog/2013/07/12/serverspec-the-new-best-way-to-learn-and-audit-your-infrastructure/">serverspec</a>, to learn what each machine does, then implementing nagios behind the serverspec scripts for a proactive monitoring on error conditions. This might seem redundant, but I don&rsquo;t think so. serverspec is great for spot checking, while nagios gives the best &ldquo;state of the union&rdquo; snapshot. Granted a lot of the time creating these checks are reactive, but sometimes like here, I wrote some proactive monitoring because I know there are certain error conditions on the slaves I&rsquo;ve spun up.</p>

<p>I use nrpe to monitor my machines, and I set up my <code>nrpe.cfg</code> to something like the following. I use chef, so the <code>&lt;%= node['hostname'] %&gt;</code> is from <a href="https://github.com/opscode/ohai">ohai</a>.</p>

<p><code>ini
command[check_redisserver]=/usr/lib/nagios/plugins/check_procs -c 1:1 -C redis-server
command[check_redis_replication]=/usr/lib/nagios/plugins/check_redis_replication -h &lt;%= node['hostname'] %&gt; -w 20 -c 45
command[check_redis_rdb_age]=/usr/lib/nagios/plugins/check_redis_rdb_age &lt;%= node['hostname'] %&gt;
</code></p>

<p>The first line is pretty self explanatory.  Obviously I&rsquo;d like to check that redis is running, and that&rsquo;s what it does.</p>

<p>The second line is where I started creating the new monitoring.  I have all my monitors right now at a 3 strike out rule, I have played with the idea of tweaking it, but it seems 3 strikes then an email seems to cut down on the noise I get.</p>

<p>```ruby</p>

<h1>!/usr/bin/env ruby</h1>

<p>require &lsquo;optparse&rsquo;
options  = {}
required = [:warning, :critical, :host]</p>

<p>parser   = OptionParser.new do |opts|</p>

<pre><code>opts.banner = "Usage: check_redis_replication [options]"
  opts.on("-h", "--host redishost", "The hostname of the redis slave") do |h|
    options[:host] = h
  end
  opts.on("-w", "--warning percentage", "Warning threshold") do |w|
    options[:warning] = w
  end
  opts.on("-c", "--critical critical", "Critical threshold") do |c|
    options[:critical] = c
 end
</code></pre>

<p>end
parser.parse!
abort parser.to_s if !required.all? { |k| options.has_key?(k) }</p>

<p>master_last_io_seconds_ago = <code>/opt/redis/bin/redis-cli info | grep master_last_io_seconds_ago</code>.split(&lsquo;:&rsquo;).last.to_i rescue -1</p>

<p>status = &ldquo;A-Ok!&rdquo;
if master_last_io_seconds_ago &lt; 0 || master_last_io_seconds_ago >= options[:critical].to_i
  status = :critical
elsif master_last_io_seconds_ago >= options[:warning].to_i
  status = :warning
end</p>

<p>status_detail = master_last_io_seconds_ago == -1 ? &lsquo;ERROR&rsquo; : &ldquo;#{ master_last_io_seconds_ago.to_s }s&rdquo;
puts &ldquo;#{status.to_s.upcase} &ndash; replication lag: #{status_detail}&rdquo;</p>

<p>if status == :critical
  exit(2)
elsif status == :warning
  exit(1)
end
```</p>

<p>I stole this monitor from <a href="http://blog.winfieldpeterson.com/2012/12/13/monitoring-redis-replication-in-nagios/">here</a> but as you can tell the <code>redis-cli</code> is edited and the default status too.  <a href="https://twitter.com/miah_">Miah</a>&rsquo;s <a href="https://github.com/miah/chef-redis">cookbook</a> is great, works like a charm, but it puts the <code>redis-cli</code> in <code>/opt/redis/bin/</code>.</p>

<p>As a side note: I even put in a <a href="https://github.com/miah/chef-redis/pull/55">PR</a> to update to 2.6.16, because of this project.  I ran all the <a href="https://github.com/opscode/test-kitchen">test-kitchen</a> tests and it passed like a charm.  I actually learned about <a href="https://github.com/sstephenson/bats">bats</a> in the process, and honestly I&rsquo;m pretty damn impressed with something so straight forward.  Because serverspec fulfills my requirements at the moment, I don&rsquo;t have to use it; but I like the idea of having it in my back pocket.  The replication between master and slave seems extremely important, so this monitor has been a god sent.  I never have seen it spike over 45 seconds, but then the next check pop back to &lt;3 seconds.</p>

<p>The second monitor I wrote was this:
```bash</p>

<h1>!/bin/bash</h1>

<p>stateid=
LAST_SYNC_HUMAN=<code>/opt/redis/bin/redis-cli -h $1 info | grep rdb_last_save_time | awk -F : {'print $2'} | xargs -I {} date -d @{}</code>
LAST_SYNC=<code>/opt/redis/bin/redis-cli -h $1 info | grep rdb_last_save_time | awk -F : {'print $2'} | tr -d '\r'</code>
LAST_15MIN=<code>date -d '15 minutes ago' +%s</code>
LAST_20MIN=<code>date -d '20 minutes ago' +%s</code></p>

<p>if [ &ldquo;${LAST_15MIN}&rdquo; -lt &ldquo;${LAST_SYNC}&rdquo; ];
  then
  echo Sync has happened in the last 15 mins or less at ${LAST_SYNC_HUMAN}
  stateid=0
elif [ &ldquo;${LAST_15MIN}&rdquo; -ge &ldquo;${LAST_SYNC}&rdquo; ] &amp;&amp; [ &ldquo;${LAST_20MIN}&rdquo; -lt &ldquo;${LAST_SYNC}&rdquo; ];
  then
  echo Sync happened a little longer than we would like, the last at ${LAST_SYNC_HUMAN}
  stateid=1
elif [ &ldquo;${LAST_20MIN}&rdquo; -ge &ldquo;${LAST_SYNC}&rdquo; ];
  then
  echo Sync took over 20 mins, something might be wrong, the last sync happened at ${LAST_SYNC_HUMAN}
  stateid=2
fi
exit $stateid
```
This monitor I took from my typical <a href="https://github.com/jjasghar/scripts/blob/master/nagios_check_generic_template.sh">bash nagios template</a> and added the error states.  It seems like the .rdb file should always be in &ldquo;close&rdquo; proximity to the last write of to slave, and it would be nice to be alarmed about it if it went out of wack.  From what I&rsquo;ve seen it&rsquo;s never more than 1 or 2 minutes behind; so this worked out.  Writing a monitor for something and never seeing it fire till there is a real problem, that&rsquo;s a great place to be in.</p>

<h2>Non-nagios monitoring</h2>

<p>I started looking around for some type of dashboard to get some visual data about the redis instance.  It turns out there is a TON of these bastards, from a paid product like <a href="https://scoutapp.com/plugin_urls/271-redis-monitoring">Scout</a> to <a href="https://github.com/junegunn/redis-stat">redis-stat</a>.  I picked redis-stat because well, it&rsquo;s un-intrusive and fast.  I have it polling against my master slave on two instances of the gem running, and it&rsquo;s really nice to have a easily digestible view of the health of it.  I have noticed that it has doubled up the info because of the syncing, so be warned, you probably should run it via just the master and then maybe put all the slaves on one? (I&rsquo;m still trying to figure this one out.)</p>

<p>If you do read this and have good suggestions please through them my <a href="http://jjasghar.github.io/about/">way</a>, the <code>INFO</code> command gives you a lot of data, and redis-stat doesn&rsquo;t store anything so treading over time isn&rsquo;t an option. I was thinking about putting some of this data in ganglia, (another favorite of mine) but to have something just redis focused seems like the correct move here.</p>

<hr />

<p>So, so far these is the monitors that I&rsquo;m using for my new master slave set up.  My next challenge is creating the VIP in front of the master slave, and have a &ldquo;hot fail over.&rdquo; This is going to require some pretty interesting engineering; it&rsquo;s fun, but man the more I think about it the more it hurts my brain.  So I haven&rsquo;t actually talked about the migration yet either.  From what I&rsquo;ve understood it seems that 2.4.16 is actually <em>inside</em> 2.6.16, so if I get this VIP in front of this master slave then fail over to the slave turning off the read-only, this should be seamless. I need to do some testing; but that seems like the best course of action. For just the migration though, the &ldquo;hot fail over&rdquo; isn&rsquo;t required, but I want it to be in the fore-front of the design because I&rsquo;d rather be additive than have to rip out everything after we get up dated.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Adventures migrating from 2.4.16 redis to 2.6.16 redis (part 1)]]></title>
    <link href="http://jjasghar.github.io/blog/2013/09/30/adventures-migrating-from-2-dot-4-16-redis-to-2-dot-6-16-redis/"/>
    <updated>2013-09-30T20:15:00-05:00</updated>
    <id>http://jjasghar.github.io/blog/2013/09/30/adventures-migrating-from-2-dot-4-16-redis-to-2-dot-6-16-redis</id>
    <content type="html"><![CDATA[<p>Like most sysadmins out there, I read the blogs and posts about new technologies that are relevant to my product(s).  There has been a lot of buzz about NoSQL and the key value store type db&rsquo;s out there, yet I have never really focused directly on it.  I have paid passing attention, until I realized that we use one heavily at my company.</p>

<p><a href="http://redis.io">Redis</a> is a key value store that runs completely in memory. It&rsquo;s fast, REALLY fast, and after spending some time with a few tutorials like <a href="http://openmymind.net/2012/1/23/The-Little-Redis-Book/">The Little Redis Book</a> by <a href="https://twitter.com/karlseguin">Karl Seguin</a>, it makes a ton of sense, and an extremely neat backend piece of software.</p>

<p>I started to learn more about it, and I started talking to a coworker and he said that he really wanted to upgrade from 2.4 redis to 2.6 redis. I took this as a challenge, and ran with it.</p>

<p>The first part of the migration is creating a slave.  We had .aof and .rdb snapshotting, but we didn&rsquo;t have a slave. This post is be going to be chronicling my adventures getting from 2.4.16 to 2.6.16.</p>

<p>Another coworker of mine, <a href="https://github.com/lwoodson">Lance Woodson</a>, created a script called &ldquo;<a href="https://gist.github.com/jjasghar/423d040444a4f4cbea1d">bloater</a>&rdquo; to start putting trash keys in the redis db.  As you can see it&rsquo;s pretty straight forward, I do strongly suggest using <code>bloater.clear!</code> if you are playing with it.  I leveraged this to simulate creating a slave already in production, the theory being I&rsquo;ll have data inside the redis db, things talking to db, and the sync happening.  I understand that a lot of people have tested this situation, but I couldn&rsquo;t seem to find any direct examples.  Hopefully this will be useful to someone.</p>

<p>The first step after getting my 2.4.16 redis machine running, I ran <code>bloater.bloat! 2295000000</code> to get an .rdb file that was ~2 gigs.  I configured the snapshotting at the default values so it created it without a hitch.  I copied it off called it something like <code>2gig-dump.rdb</code> or something.  Then I ran <code>bloater.bloat! 4295000000</code> to create the 4gig dump file.  I copied that off next and then I had my two dumpfiles that I could play with.  You are probably wondering why 2 and 4 gigs, and also &ldquo;Dang, that&rsquo;s a huge redis db.&rdquo; Yes, yes it is, I focused on the &ldquo;oh crap, our db is too big&rdquo; situation, so if redis handles that, then I know it&rsquo;ll deal with the 100 meg db just fine.</p>

<p>Secondly I did my <code>FLUSHALL</code> command, then shutdown my redis instance then copied the 2 gig db, in the <code>/var/lib/redis/</code> location.  I spun up redis, and ran the <code>INFO</code> command, like all redis admins learn to love to do.  I saw that it was 2 gigs, and yay, now I had my foundation to start my testing.</p>

<p>I went through quite a few different iterations, but I&rsquo;ll just explain the basic trouble shooting that I did, and maybe you can leverage off it.</p>

<p>So with my redis 2.4 instance with a 2 gig db, and my 2.6 blank db instance it was time to start the fun.  I ran the following commands in a tmux session on my main workstation.  I liked the idea of a 3rd party machine actually looking at all this stuff instead of running directly on one of the machines.
<code>bash
watch "redis-cli -h redis-slave info | grep master_sync_left_bytes "
redis-cli -h redis-slave keys "*" | wc -l
redis-cli -h redis-master keys "*" | wc -l
redis-cli -h redis-master info | grep used_memory
redis-cli -h redis-slave info | grep used_memory
redis-cli -h redis-master --latency
</code></p>

<p>Most, if not all are pretty self explanatory.  The most interesting one of them all is probably the last one, the <code>--latency</code>.  It used it as a way to emulate something poking my redis instance during the inital clone. The thing we, [where|are|still] worried about is customer impact, and as long as the number didn&rsquo;t sky rocket then stay high it was an acceptable risk.</p>

<p>After I got the tmux session all up and happy, i  ran these two commands to start the <code>slaveof</code>:
```bash</p>

<h1>on the master</h1>

<p>redis-master:6379> slaveof no one</p>

<h1>on the slave</h1>

<p>redis-slave:6379> slaveof redis-master 6379
```</p>

<p>It worked like a charm. The <code>watch "redis-cli -h redis-slave info | grep master_sync_left_bytes "</code> started showing me the amount of data left for the transfer, and before I knew it I had a working master slave configuration.  This was a lot of cruft to basically say <code>slaveof redis-master 6379</code> but hell it&rsquo;s sometimes nice to see the thought process around it.  Oh!, the latency, did spike to 800 from .05, but I discovered that was in milliseconds and only ONE sample.  So yeah one call to the master with a spike of something still sub-second, was still in the risk params.
Now I have a working master slave configuration.  Next (part 2) I&rsquo;ll post about my monitoring of this, it&rsquo;s not <em>that</em> exciting, but it is useful.</p>
]]></content>
  </entry>
  
</feed>
