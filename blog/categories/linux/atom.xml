<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | jjasghar rants and ramblings]]></title>
  <link href="http://jjasghar.github.io/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://jjasghar.github.io/"/>
  <updated>2013-09-30T20:31:31-05:00</updated>
  <id>http://jjasghar.github.io/</id>
  <author>
    <name><![CDATA[JJ Asghar]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Adventures migrating from 2.4.16 redis to 2.6.16 redis (part 1)]]></title>
    <link href="http://jjasghar.github.io/blog/2013/09/30/adventures-migrating-from-2-dot-4-16-redis-to-2-dot-6-16-redis/"/>
    <updated>2013-09-30T20:15:00-05:00</updated>
    <id>http://jjasghar.github.io/blog/2013/09/30/adventures-migrating-from-2-dot-4-16-redis-to-2-dot-6-16-redis</id>
    <content type="html"><![CDATA[<p>Like most sysadmins out there, I read the blogs and posts about new technologies that are relevant to my product(s).  There has been a lot of buzz about NoSQL and the key value store type db&rsquo;s out there, yet I have never really focused directly on it.  I have paid passing attention, until I realized that we use one heavily at my company.</p>

<p><a href="http://redis.io">Redis</a> is a key value store that runs completely in memory. It&rsquo;s fast, REALLY fast, and after spending some time with a few tutorials like <a href="http://openmymind.net/2012/1/23/The-Little-Redis-Book/">The Little Redis Book</a> by <a href="https://twitter.com/karlseguin">Karl Seguin</a>, it makes a ton of sense, and an extremely neat backend piece of software.</p>

<p>I started to learn more about it, and I started talking to a coworker and he said that he really wanted to upgrade from 2.4 redis to 2.6 redis. I took this as a challenge, and ran with it.</p>

<p>The first part of the migration is creating a slave.  We had .aof and .rdb snapshotting, but we didn&rsquo;t have a slave. This post is be going to be chronicling my adventures getting from 2.4.16 to 2.6.16.</p>

<p>Another coworker of mine, <a href="https://github.com/lwoodson">Lance Woodson</a>, created a script called &ldquo;<a href="https://gist.github.com/jjasghar/423d040444a4f4cbea1d">bloater</a>&rdquo; to start putting trash keys in the redis db.  As you can see it&rsquo;s pretty straight forward, I do strongly suggest using <code>bloater.clear!</code> if you are playing with it.  I leveraged this to simulate creating a slave already in production, the theory being I&rsquo;ll have data inside the redis db, things talking to db, and the sync happening.  I understand that a lot of people have tested this situation, but I couldn&rsquo;t seem to find any direct examples.  Hopefully this will be useful to someone.</p>

<p>The first step after getting my 2.4.16 redis machine running, I ran <code>bloater.bloat! 2295000000</code> to get an .rdb file that was ~2 gigs.  I configured the snapshotting at the default values so it created it without a hitch.  I copied it off called it something like <code>2gig-dump.rdb</code> or something.  Then I ran <code>bloater.bloat! 4295000000</code> to create the 4gig dump file.  I copied that off next and then I had my two dumpfiles that I could play with.  You are probably wondering why 2 and 4 gigs, and also &ldquo;Dang, that&rsquo;s a huge redis db.&rdquo; Yes, yes it is, I focused on the &ldquo;oh crap, our db is too big&rdquo; situation, so if redis handles that, then I know it&rsquo;ll deal with the 100 meg db just fine.</p>

<p>Secondly I did my <code>FLUSHALL</code> command, then shutdown my redis instance then copied the 2 gig db, in the <code>/var/lib/redis/</code> location.  I spun up redis, and ran the <code>INFO</code> command, like all redis admins learn to love to do.  I saw that it was 2 gigs, and yay, now I had my foundation to start my testing.</p>

<p>I went through quite a few different iterations, but I&rsquo;ll just explain the basic trouble shooting that I did, and maybe you can leverage off it.</p>

<p>So with my redis 2.4 instance with a 2 gig db, and my 2.6 blank db instance it was time to start the fun.  I ran the following commands in a tmux session on my main workstation.  I liked the idea of a 3rd party machine actually looking at all this stuff instead of running directly on one of the machines.
<code>bash
watch "redis-cli -h redis-slave info | grep master_sync_left_bytes "
redis-cli -h redis-slave keys "*" | wc -l
redis-cli -h redis-master keys "*" | wc -l
redis-cli -h redis-master info | grep used_memory
redis-cli -h redis-slave info | grep used_memory
redis-cli -h redis-master --latency
</code></p>

<p>Most, if not all are pretty self explanatory.  The most interesting one of them all is probably the last one, the <code>--latency</code>.  It used it as a way to emulate something poking my redis instance during the inital clone. The thing we, [where|are|still] worried about is customer impact, and as long as the number didn&rsquo;t sky rocket then stay high it was an acceptable risk.</p>

<p>After I got the tmux session all up and happy, i  ran these two commands to start the <code>slaveof</code>:
```bash</p>

<h1>on the master</h1>

<p>redis-master:6379> slaveof no one</p>

<h1>on the slave</h1>

<p>redis-slave:6379> slaveof redis-master 6379
```</p>

<p>It worked like a charm. The <pre>watch &ldquo;redis-cli -h redis-slave info | grep master_sync_left_bytes &rdquo;</pre> started giving me data, and before I knew it I had a working master slave configuration.  This was a lot of cruft to basically say <code>slaveof redis-master 6379</code> but hell it&rsquo;s sometimes nice to see the thought process around it.  Oh!, the latency, did spike to 800 from .05, but I discovered that was in milliseconds and only ONE sample.  So yeah one call to the master with a spike of something still sub-second, was still in the risk params.
Now I have a working master slave configuration.  Next (part 2) I&rsquo;ll post about my monitoring of this, it&rsquo;s not <em>that</em> exciting, but it is useful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[a bash numbering trick I just learned today]]></title>
    <link href="http://jjasghar.github.io/blog/2013/09/11/a-bash-numbering-trick-i-just-learned-today/"/>
    <updated>2013-09-11T13:05:00-05:00</updated>
    <id>http://jjasghar.github.io/blog/2013/09/11/a-bash-numbering-trick-i-just-learned-today</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been using bash for&hellip;a really really long time.  I&rsquo;ve always done <code>seq 1 10</code> in my for loops to get <code>1 2 3 4 5 6 7 8 9 10</code>.  Today I learned these three ways do the exact same thing. (Note: bash v4 above) I&rsquo;m writing this out, mainly because I want to remind myself of this again one day.
```bash
for ((i=1; i&lt;=10; i++)); do echo $i; done</p>

<p>for i in <code>seq 1 10</code> ; do echo $i; done</p>

<p>for i in {1..10} ; do echo $i; done
```
Dang you really do learn something new every day.</p>

<p>Note: A good friend of mine <a href="https://github.com/awaxa/">awaxa</a> pointed out that {1..10} is the fastest in this case because it&rsquo;s a built into bash.  Thanks awaxa!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Adventures with chef-solo]]></title>
    <link href="http://jjasghar.github.io/blog/2013/08/02/adventures-with-chef-solo/"/>
    <updated>2013-08-02T14:36:00-05:00</updated>
    <id>http://jjasghar.github.io/blog/2013/08/02/adventures-with-chef-solo</id>
    <content type="html"><![CDATA[<p>I started my chef career with inheriting a chef infrastructure with an opensource chef server.  I read all of the blog posts, and the tutorials, tried to wrap my head around what a cookbook, recipe, or data bag was.  I know now, this was the <em>wrong</em> way to learn chef.  chef with chef-client/server adds a level of complexity that just is unfounded.  I decided to spend a copy hours to master chef-solo, and damn I wish I had started there.</p>

<h2>My hands on learning project</h2>

<p>I had a requirement of setting up a unicorn nginx instance on a VPS.  I thought hell, there&rsquo;s gotta be a way to do this with chef. (I normally use passenger+apache, so I had to learn nginx and unicorn too, but that&rsquo;s a different post&hellip;well kinda) So I looked around for a nice <a href="http://www.opinionatedprogrammer.com/2011/06/chef-solo-tutorial-managing-a-single-server-with-chef/">tutorial</a> and came across that guy.  I started reading it, realized holy crap, this is awesome.  I stole the <code>solo.json</code> and the <code>solo.rb</code> and <code>install.sh</code> because I was lazy.<br/>
I changed the <code>install.sh</code> around a little bit:
```bash</p>

<h1>!/bin/bash</h1>

<h1>This run as root on the machine</h1>

<p>chef_binary=/opt/chef/embedded/lib/ruby/gems/1.9.1/gems/chef-11.6.0/bin/chef-solo</p>

<h1>are we not bootstrapped?</h1>

<p>if ! test -f &ldquo;$chef_binary&rdquo;; then
  export DEBIAN_FRONTEND=noniteractive
  apt-get update &amp;&amp;
  apt-get dist-upgrade -y &amp;&amp;
  apt-get install ruby1.9.1 ruby1.9.1-dev make curl -y &amp;&amp;
  curl -L <a href="https://www.opscode.com/chef/install.sh">https://www.opscode.com/chef/install.sh</a> | sudo bash
fi &amp;&amp;</p>

<p>chef-solo -c solo.rb -j solo.json
<code>``
As you can tell, its ubuntu focused, but basically if I want to provision a box after making a change to one of the cookbooks I just had to run</code>./install.sh`.  Interestingly, I had heard bad things about the 1.9.1 ruby binaries for ubuntu, but so far, it is a ton better than doing that compile from src. ;)</p>

<p>So, basically with those three files, running that curl in the script, and creating a <code>cookbooks/</code> directory, I had my framework to make this project happen.</p>

<p>Now I made a mistake here.  I started off with a simple &ldquo;generic&rdquo; cookbook, with a simple <code>default.rb</code> file that called a <code>packages.rb</code> file to add some pkgs I love and run on a daily basis.</p>

<p>```ruby
%w{build-essential git-core libgdbm-dev libreadline-dev libssl-dev libyaml-dev s3cmd tmux tk-dev vim wget zlib1g-dev zsh}.each do |pkg|
  package pkg do</p>

<pre><code>action [:install]
</code></pre>

<p>  end
end
```</p>

<p>Now to an passive onlooker there seems nothing wrong with this, and 3 weeks ago JJ would agree with you.  JJ now says, &ldquo;Stop that, start with the webserver and the deps for that instead of the convince of something easy like package management.&rdquo;  LEARN <a href="http://berkshelf.com/">berkshelf</a> and don&rsquo;t make excuses. (at this writing, 8/2/13, I still actually haven&rsquo;t learned it, the best analogy is this, dling cookbooks is like building from source, and berkshelf is like using apt-get) Anyway I digress.</p>

<p>Back to the original story.</p>

<p>I made the <code>cookbooks/</code> directory, and when to github to pull down the newest <a href="https://github.com/opscode-cookbooks/nginx">nginx</a> cookbook.  <code>git clone https://github.com/opscode-cookbooks/nginx.git</code> in the directory and then because  I hate submodules, I ran <code>rm -rf .git/</code>. I <code>cd ~/chef-solo</code> then ran <code>install.sh</code>. Crossed my fingers and no, didn&rsquo;t work. Turns out I needed a bunch of dependences (hence the berkself statement). I resolved them all, and then ran the beautiful <code>./install.sh</code>. BOOM, it worked.  I took a moment, realized what I had just done.  I created a git repo off this work, created a branch &ldquo;nginx&rdquo; and committed my changes. Now whenever I need to spin up an nginx box, all I have to do is clone my chef-solo repo and checkout nginx and run <code>install.sh</code>.</p>

<p>Dang, that&rsquo;s awesome. To get that to work with chef-server&hellip;is a ton of work.  You have to download each cookbook, upload each to the chef-server, change the role or add the recipe, then run chef-client.  This would be great if I had to do it over and over, but being I was just setting up a dev box, chef-server overhead just isn&rsquo;t worth it.</p>

<p>Now all in all, this has only been one adventure with chef-solo, but going through this academic exercise I realized the potential of this unbelievable powerful application.  From this project, and my &ldquo;generic&rdquo; cookbook, I&rsquo;ve ran with it, so now I have a boostrap cookbook with my packages, some of my dotfiles, and other tidbits.  It&rsquo;s nice, and I strongly suggest checking it out.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[semver or how I learned that versioning everywhere where I've worked is stupid and semver is the way to go]]></title>
    <link href="http://jjasghar.github.io/blog/2013/07/25/semver-or-how-i-learned-that-versioning-everywhere-where-ive-worked-is-stupid-and-semver-is-the-way-to-go/"/>
    <updated>2013-07-25T16:37:00-05:00</updated>
    <id>http://jjasghar.github.io/blog/2013/07/25/semver-or-how-i-learned-that-versioning-everywhere-where-ive-worked-is-stupid-and-semver-is-the-way-to-go</id>
    <content type="html"><![CDATA[<p>In the chef world the way that cookbooks are named versioned is something called semver.  Before i watched Jamie Winsor&rsquo;s <a href="https://www.youtube.com/watch?v=hYt0E84kYUI">Berkself Way</a> I didn&rsquo;t know understand how the versioning worked.  I quickly discovered <a href="http://www.semver.org">http://www.semver.org</a> and fell in love.</p>

<p>TODO.</p>

<p>Seriously, though it really is awesome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[serverspec the new best way to learn and audit your infrastructure]]></title>
    <link href="http://jjasghar.github.io/blog/2013/07/12/serverspec-the-new-best-way-to-learn-and-audit-your-infrastructure/"/>
    <updated>2013-07-12T15:06:00-05:00</updated>
    <id>http://jjasghar.github.io/blog/2013/07/12/serverspec-the-new-best-way-to-learn-and-audit-your-infrastructure</id>
    <content type="html"><![CDATA[<p>Listening to the <a href="http://www.opscode.com">Opscode</a> comunity pod cast, the <a href="https://twitter.com/foodfightshow">foodfight</a> show, <a href="https://twitter.com/bryanwb">bryanwb</a> suggested more love for something called <a href="http://serverspec.org">serverspec</a>.  A couple days later he wrote a <a href="https://twitter.com/bryanwb/status/340155947634794496">tweet</a> and I figured I&rsquo;d give it a shot.  I&rsquo;ve been blown away by it; and I&rsquo;m going to sell/leverage it where ever I can.</p>

<p>serverspec is basically <a href="http://rspec.info/">rspec</a> for sysadmins, from what I&rsquo;ve tested basic rspec (and some advanced rspec) assertions work like a charm. Here&rsquo;s a <a href="https://gist.github.com/byplayer/965857">cheat sheet</a> if you want.  You can write tests like ruby developers do, but leverage them for your infrastructure.  As I&rsquo;ve been learning more about TDD, I see the advantage, but to put that style in sysadmining, might seem initially crazy.</p>

<p>As a new sysadmin, you&rsquo;ve probably had an experience something like this:
You come in at a new job, you have the initial &ldquo;Hey newguy, here&rsquo;s your new network.  You have web machines, databases, app servers, caching servers, configuration management and other boxes. We have process monitoring <em>ahem</em>nagios<em>ahem</em> and yes we have configuration management where you have to leverage code reviews to get anything out to production.&rdquo; As a new guy I&rsquo;m excited this is great, but here comes the kicker.  Your new trainer continues, &ldquo;Ok, lets walk through each machine.  There are 10 web servers, 9 of them are called www1 through www8, and one called paco.&rdquo; You quickly grab a piece of paper, &ldquo;Paco?&rdquo;, &ldquo;Yes the admin before you didn&rsquo;t like standard names, he wanted to give machines &lsquo;personality&rsquo;, and we had a management change after him, so we have that one off.&rdquo;</p>

<p>This is where serverspec comes into play.  You know that list that starts to grow, learning about your app servers, your one off web machines? If you use serverspec, you can keep the pseudo-english list and be able to check against what should be on there. It  gives you a way to confirm everything is exactly what you expect.</p>

<p>serverspec only requires ruby to run, the following command will get it installed if you have ruby 1.9.x+ installed. <code>gem install serverspec</code> and ssh access to the boxes you want to check.  The example of the webserver on the main page is priceless and speaks volumes. The following is that example with a small edit from me.
``` ruby
require &lsquo;spec_helper&rsquo;</p>

<p>describe package(&lsquo;httpd&rsquo;) do
  it { should be_installed }
end</p>

<p>describe service(&lsquo;httpd&rsquo;) do
  it { should be_enabled   }
  it { should be_running   }
end</p>

<p>describe port(80) do
  it { should be_listening }
end</p>

<p>describe file(&lsquo;/etc/httpd/conf/httpd.conf&rsquo;) do
  it { should be_file }
  it { should contain &ldquo;ServerName my-server-name&rdquo; }
end
<code>``
That's it, now all you have to do is</code>rake spec` and you have a test that checks for all the basic apache web server stuff, and instead of sshing out to each machine you can make the computer do the work for you, and this is just the tip of the iceberg!</p>

<h2>nagios (or automated process monitoring) vs serverspec</h2>

<p>I understand the hesitation about <em>another</em> monitoring solution, but every sysadmin goes through this situation like this when learning a new network.  Like the story above, there should be at least one rudimentary monitoring solution, a way to  make sure machines are up and maybe processes aren&rsquo;t erring.  With the low overhead of serverspec you can leverage this in an ad-hoc way and, I&rsquo;d like to state that this isn&rsquo;t designed to take place of something like nagios, but to augment it.</p>

<p>The advantage of something like serverspec over nagios or, a polling based monitoring solution, is that there is a lag between when you deploy something and the monitoring systems reporting it.  You can run serverspec in an ad-hoc manner you&rsquo;ll get a much faster feedback loop.  I have 1034 tests on about 40 boxes, and it only takes about 2 minutes 30 seconds to run, needless to say that&rsquo;s a small price to pay for a piece of mind.  The default polling for nagios is 5 minutes, so cutting it in half that&rsquo;s already a great turn around to see the state of the machines.</p>

<p>I&rsquo;ve also able to get serverspec working with jenkins, and run the <code>rake</code> task every 30 minutes, and if it exits on anything other than a 0, it&rsquo;ll email out the configuration or error that it finds. I&rsquo;m still experimenting with jenkins and serverspec, but so far it&rsquo;s nice to have something other than cron running this.</p>

<h2>Conclusion</h2>

<p>serverspec is a tool/gem that I&rsquo;ll be leveraging from now on.  I&rsquo;ve been wanting to learn rspec and this gives me an avenue to use it day to day; and get a huge benefit from it.  I even bought <a href="http://pragprog.com/book/achbd/the-rspec-book">The RSpec Book: Behaviour-Driven Development with RSpec, Cucumber, and Friends</a> by The Pragmatic Programmers, so hopefully I&rsquo;ll be able to leverage it to get to the next level. Give it a shot, and when you get all your green dots come back, you&rsquo;ll know you&rsquo;ve found a winner.</p>
]]></content>
  </entry>
  
</feed>
